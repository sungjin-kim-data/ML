{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOorhKJ6ZsS2cosV6e3OUhL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sungjin-kim-data/ML/blob/master/ridge%2C_lasso_regerssion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOYieoGOILT-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ridge Regression\n",
        "\n",
        "- Ridge Regression은 회귀계수에 `가중치들의 제곱합(L2 penalty)`을 패널티로 부과하여 회귀계수의 크기를 줄이는 모델입니다.\n",
        "- L2 패널티를 적용하면 영향력이 크지 않은 회귀계수의 값은 0에 가까운 수로 축소합니다.\n",
        "- Ridge Regression의 비용함수는 다음과 같습니다.\n",
        "\n",
        "$$ cost = RSS + \\lambda\\sum_{j=1}^p\\beta_j^2 = \\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1x_{i1}-\\dotsc-\\beta_px_{ip})^2 + \\lambda\\sum_{j=1}^p\\beta_j^2 $$\n"
      ],
      "metadata": {
        "id": "1rtUnkUU9pIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lasso Regression\n",
        "\n",
        "- Lasso Regression은 회귀계수에 `가중치들의 절대값의 합(L1 penalty)`을 패널티로 부과하여 회귀계수의 크기를 줄이는 모델입니다.\n",
        "- Lasso의 중요한 특징은 영향력이 크지 않은 회귀계수의 값을 0으로 만든다는 것입니다.\n",
        "    - 그 결과 자동으로 특성을 선택하는 효과를 가지게 되며 희소모델(sparse model)을 만들게 됩니다. \n",
        "    - 희소 모델이란 가중치가 0인 특성이 많은 모델을 말합니다.\n",
        "- Lasso Regression의 비용함수는 다음과 같습니다.\n",
        "\n",
        "$$ cost = RSS + \\lambda\\sum_{j=1}^p\\vert\\beta_j\\vert = \\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1x_{i1}-\\dotsc-\\beta_px_{ip})^2 + \\lambda\\sum_{j=1}^p\\vert\\beta_j\\vert $$"
      ],
      "metadata": {
        "id": "BaukxyVc9yp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\lambda$ (Lambda)\n",
        "- $\\lambda$ 는 패널티의 강도를 조절하는 하이퍼파라미터입니다. lambda, alpha, penalty term, regularization term 등으로 불립니다.\n",
        "- $\\lambda$ 의 크기가 클수록 회귀계수의 값이 줄어듭니다. \n",
        "    - $\\lambda = 0$ 인 경우 기존의 선형회귀와 같아집니다.\n",
        "    - $\\lambda = \\infty $ 인 경우  $\\beta = 0$ 이 됩니다.\n"
      ],
      "metadata": {
        "id": "m57zdz5X90T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ans = sns.load_dataset('anscombe').query('dataset==\"III\"')\n",
        "ans.plot.scatter('x', 'y');"
      ],
      "metadata": {
        "id": "-ig8MlJ_95WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ridge regression 구현\n",
        "ridge = Ridge(alpha=0.5, normalize=True) # alpha : 패널티, normalize : 표준화 작업\n",
        "ridge.fit(ans[['x']], ans['y'])\n",
        "y_pred_ridge = ridge.predict(ans[['x']])\n",
        "\n",
        "# 그래프 그리기\n",
        "ans.plot.scatter('x', 'y')\n",
        "plt.plot(ans['x'], y_pred_ridge)\n",
        "plt.title(f'y= {ridge.coef_[0].round(2)}x + {ridge.intercept_.round(2)}')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qIs1UDem95ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lasso regression 구현\n",
        "lasso = Lasso(alpha=0.1, normalize=True) # alpha : 패널티, normalize : 표준화 작업\n",
        "lasso.fit(ans[['x']], ans['y'])\n",
        "y_pred_lasso = lasso.predict(ans[['x']])\n",
        "\n",
        "\n",
        "# 그래프 그리기\n",
        "ans.plot.scatter('x', 'y')\n",
        "plt.plot(ans['x'], y_pred_lasso)\n",
        "plt.title(f'y= {lasso.coef_[0].round(2)}x + {lasso.intercept_.round(2)}')\n",
        "plt.show()ㅠ"
      ],
      "metadata": {
        "id": "mX20UzIj95b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프를 그리기 위한 함수\n",
        "def predict_ypred(alpha, model) : \n",
        "    ml = model(alpha=alpha, normalize=True)\n",
        "    ml.fit(ans[['x']], ans['y'])\n",
        "    m = ml.coef_[0].round(2)\n",
        "    b = ml.intercept_.round(2)\n",
        "    formula = f'y = {m}x + {b}'\n",
        "    ans['y_pred'] = ml.predict(ans[['x']])\n",
        "    return ans['y_pred'], formula\n",
        "\n",
        "\n",
        "def plotting(model, alphas, ax, title, legend) :\n",
        "    colors = ['blue', 'red','orange', 'green']\n",
        "    for alpha, color in zip(alphas, colors):\n",
        "        ans['y_pred'], formula = predict_ypred(alpha, model)\n",
        "        ans.plot('x', 'y_pred', ax=ax, color =color, linestyle='dashed', alpha=0.5)\n",
        "        ax.text(14, ans.loc[27, 'y_pred'], formula)\n",
        "    ax.set_title(title)\n",
        "    ax.legend(legend)"
      ],
      "metadata": {
        "id": "TjZirShh99XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "ans = sns.load_dataset('anscombe').query('dataset==\"III\"')\n",
        "ans = ans.sort_values(by='x')\n",
        "\n",
        "fig = plt.figure(figsize=(30, 6))\n",
        "ax = fig.add_subplot(131)\n",
        "ax.scatter(ans['x'], ans['y'])\n",
        "\n",
        "\"\"\"\n",
        "OLS\n",
        "\"\"\"\n",
        "ols = LinearRegression()\n",
        "ols.fit(ans[['x']], ans['y'])\n",
        "\n",
        "# 회귀계수와 intercept를 확인합니다.\n",
        "m = ols.coef_[0].round(2)\n",
        "b = ols.intercept_.round(2)\n",
        "title = f'Linear Regression \\n y = {m}x + {b}'\n",
        "\n",
        "# 훈련 데이터로 예측을 합니다.\n",
        "ans['y_pred'] = ols.predict(ans[['x']])\n",
        "ans.plot('x', 'y_pred', ax=ax, title=title);\n",
        "\n",
        "\"\"\"\n",
        "Ridge Regression\n",
        "\"\"\"\n",
        "ax1 = fig.add_subplot(132)\n",
        "ax1.scatter(ans['x'], ans['y'])\n",
        "\n",
        "alphas = np.arange(0, 2, 0.5)\n",
        "title = 'Ridge Regression'\n",
        "legend = ['ancombe','alpha : 0.0', 'alpha:0.5', 'alpha:1.0', 'alpha:1.5']\n",
        "plotting(Ridge, alphas, ax1, title, legend)\n",
        "\n",
        "\"\"\"\n",
        "Lasso Regression\n",
        "\"\"\"\n",
        "ax2 = fig.add_subplot(133)\n",
        "ax2.scatter(ans['x'], ans['y'])\n",
        "\n",
        "alphas = np.arange(0, 0.4, 0.1)\n",
        "title = 'Lasso Regression'\n",
        "legend = ['ancombe','alpha : 0.0', 'alpha:0.1', 'alpha:0.2', 'alpha:0.3']\n",
        "plotting(Lasso, alphas, ax2, title, legend)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_An0HYhQ9_oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "그래프를 보면,  \n",
        "alpha = 0인 경우에는 Ridge와 Lasso 모두 OLS 와 같은 그래프 형태로 같은 모델임을 확인 할 수 있고.\n",
        "alpha 값이 커질 수록 직선의 기울기가 0에 가까워지는 것을 알 수 있습니다.\n",
        "정규화모델은 OLS 모델에 비해 이상치에 영향을 덜 받는 것을 알 수 있습니다."
      ],
      "metadata": {
        "id": "13yPmPNh-FPY"
      }
    }
  ]
}